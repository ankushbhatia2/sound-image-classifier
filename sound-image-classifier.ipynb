{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "max_pad_len, mfcc_features = 50, 13\n",
    "def wav2mfcc(file_path, max_pad_len=50):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    wave = wave[::3]\n",
    "    mfcc = librosa.feature.mfcc(wave, sr=8000, n_mfcc=mfcc_features, hop_length=160, n_fft=800)\n",
    "    mfcc = mfcc.T\n",
    "    pad_width = max_pad_len - mfcc.shape[0]\n",
    "    mfcc = np.pad(mfcc, pad_width=((0, pad_width), (0, 0)), mode='constant')\n",
    "    #print(mfcc.shape)\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 50, 13)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfccs, labels = [], []\n",
    "recordings_path = \"sound-mnist/recordings/\"\n",
    "for f in os.listdir(recordings_path):\n",
    "    mfccs.append(wav2mfcc(recordings_path + f))\n",
    "    label = f.split('_')[0]\n",
    "    labels.append(label)\n",
    "mfccs = librosa.util.normalize(np.asarray(mfccs))\n",
    "mfccs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "img_width, img_height = 28, 28\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_train_image, X_train_sound, Digits, Y_train = [], [], [], []\n",
    "X_test_image, X_test_sound, Y_test = [], [], []\n",
    "check_ind = set()\n",
    "train_set = 20000\n",
    "test_set = 2000\n",
    "\n",
    "#Training Data\n",
    "t, f = 0, 0\n",
    "while t < 11589 or f < 8411:\n",
    "    sound_ind = np.random.randint(mfccs.shape[0])\n",
    "    image_ind = np.random.randint(x_train.shape[0])\n",
    "    if (sound_ind, image_ind) in check_ind:\n",
    "        continue\n",
    "    if int(labels[sound_ind]) == int(y_train[image_ind]) and t < 11589: #Chose a random number of True Cases for training\n",
    "        Y_train.append(int(labels[sound_ind]) == int(y_train[image_ind]))\n",
    "        X_train_image.append(x_train[image_ind])\n",
    "        X_train_sound.append(mfccs[sound_ind])\n",
    "        Digit.append(int(labels[sound_ind]))\n",
    "        check_ind.add((sound_ind, image_ind))\n",
    "        t+=1\n",
    "    if int(labels[sound_ind]) != int(y_train[image_ind]) and f < 8411:\n",
    "        Y_train.append(int(labels[sound_ind]) == int(y_train[image_ind]))\n",
    "        X_train_image.append(x_train[image_ind])\n",
    "        X_train_sound.append(mfccs[sound_ind])\n",
    "        Digit.append(int(labels[sound_ind]))\n",
    "        check_ind.add((sound_ind, image_ind))\n",
    "        f+=1                               \n",
    "\n",
    "#Testing Data\n",
    "t, f = 0, 0\n",
    "while t < 562 or f < 1438:\n",
    "    sound_ind = np.random.randint(mfccs.shape[0])\n",
    "    image_ind = np.random.randint(x_train.shape[0])\n",
    "    if (sound_ind, image_ind) in check_ind:\n",
    "        continue\n",
    "    if int(labels[sound_ind]) == int(y_train[image_ind]) and t < 562:   #Chose a random number of True cases for testing\n",
    "        Y_test.append(int(labels[sound_ind]) == int(y_train[image_ind]))\n",
    "        X_test_image.append(x_train[image_ind])\n",
    "        X_test_sound.append(mfccs[sound_ind])\n",
    "        check_ind.add((sound_ind, image_ind))\n",
    "        t+=1\n",
    "    if int(labels[sound_ind]) != int(y_train[image_ind]) and f < 1438:\n",
    "        Y_test.append(int(labels[sound_ind]) == int(y_train[image_ind]))\n",
    "        X_test_image.append(x_train[image_ind])\n",
    "        X_test_sound.append(mfccs[sound_ind])\n",
    "        check_ind.add((sound_ind, image_ind))\n",
    "        f+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11589, 562)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.count(True), Y_test.count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_image = np.asarray(X_train_image).reshape(train_set, img_width, img_height, 1) / 255\n",
    "X_train_sound = np.asarray(X_train_sound).reshape(train_set, max_pad_len, mfcc_features, 1)\n",
    "Y_train = to_categorical(np.asarray(Y_train))\n",
    "\n",
    "X_test_image = np.asarray(X_test_image).reshape(test_set, img_width, img_height, 1) / 255\n",
    "X_test_sound = np.asarray(X_test_sound).reshape(test_set, max_pad_len, mfcc_features, 1)\n",
    "Y_test = to_categorical(np.asarray(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, concatenate, Add\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "image_model = Sequential()\n",
    "image_model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "image_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "image_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "image_model.add(Dropout(0.25))\n",
    "image_model.add(Flatten())\n",
    "image_model.add(Dense(128, activation='relu'))\n",
    "image_model.add(Dropout(0.5))\n",
    "#image_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#              optimizer=keras.optimizers.Adadelta(),\n",
    "#              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_model = Sequential()\n",
    "input_shape2 = (50, 13, 1)\n",
    "sound_model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=input_shape2))\n",
    "sound_model.add(BatchNormalization())\n",
    "\n",
    "sound_model.add(Conv2D(48, kernel_size=(2, 2), activation='relu'))\n",
    "sound_model.add(BatchNormalization())\n",
    "\n",
    "sound_model.add(Conv2D(120, kernel_size=(2, 2), activation='relu'))\n",
    "sound_model.add(BatchNormalization())\n",
    "\n",
    "sound_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "sound_model.add(Dropout(0.25))\n",
    "\n",
    "sound_model.add(Flatten())\n",
    "\n",
    "sound_model.add(Dense(256, activation='relu'))\n",
    "sound_model.add(BatchNormalization())\n",
    "sound_model.add(Dropout(0.25))\n",
    "sound_model.add(Dense(128, activation='relu'))\n",
    "sound_model.add(BatchNormalization())\n",
    "sound_model.add(Dropout(0.4))\n",
    "#sound_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Add()([image_model.output, sound_model.output])\n",
    "model = Dense(128, activation='relu')(model)\n",
    "model = Dropout(.35)(model)\n",
    "\n",
    "model = Dense(2, activation='softmax')(model)\n",
    "\n",
    "final_model = Model([image_model.input, sound_model.input], model)\n",
    "from keras.optimizers import SGD\n",
    "opt = SGD(lr=0.001)\n",
    "final_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.adadelta(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 77s 4ms/step - loss: 0.7966 - acc: 0.5322\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 73s 4ms/step - loss: 0.6926 - acc: 0.5700\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 77s 4ms/step - loss: 0.6295 - acc: 0.6574\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.5624 - acc: 0.7239\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 74s 4ms/step - loss: 0.5149 - acc: 0.7599\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.4645 - acc: 0.7935\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 75s 4ms/step - loss: 0.4101 - acc: 0.8248\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.3608 - acc: 0.8543\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 77s 4ms/step - loss: 0.3093 - acc: 0.8826\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 80s 4ms/step - loss: 0.2743 - acc: 0.8995\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 75s 4ms/step - loss: 0.2440 - acc: 0.9123\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 79s 4ms/step - loss: 0.2268 - acc: 0.9189\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.2038 - acc: 0.9268\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 78s 4ms/step - loss: 0.1862 - acc: 0.9334\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 80s 4ms/step - loss: 0.1679 - acc: 0.9394\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 80s 4ms/step - loss: 0.1585 - acc: 0.9425\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 77s 4ms/step - loss: 0.1402 - acc: 0.9491\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 77s 4ms/step - loss: 0.1214 - acc: 0.9564\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.1124 - acc: 0.9602\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 76s 4ms/step - loss: 0.1026 - acc: 0.9633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28cdb92eb00>"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.fit([X_train_image, X_train_sound], Y_train, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 6s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07816050169244408, 0.972]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.evaluate([X_test_image, X_test_sound], Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.save(\"C:/Users/ankubhat/Desktop/codes/Notebooks/mymodel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
